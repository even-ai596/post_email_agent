# æ·±åº¦è¿½è¸ª invoke æ–¹æ³•çš„å…·ä½“æ‰§è¡Œä»£ç 

from langchain_core.tools import BaseTool
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field
from typing import Type
import json
from unittest.mock import patch
import inspect

# 1. å®šä¹‰å·¥å…·
class WeatherInput(BaseModel):
    city: str = Field(description="åŸå¸‚åç§°")

class WeatherTool(BaseTool):
    name: str = "get_weather"
    description: str = "è·å–å¤©æ°”ä¿¡æ¯"
    args_schema: Type[BaseModel] = WeatherInput
    
    def _run(self, city: str):
        return f"{city}çš„å¤©æ°”æ˜¯æ™´å¤©ï¼Œ25åº¦"

# 2. åˆ›å»ºæ¨¡å‹å’Œç»‘å®šå·¥å…·
weather_tool = WeatherTool()
llm = ChatOpenAI(model="gpt-4o-mini")
llm_with_tools = llm.bind_tools([weather_tool])

print("=" * 100)
print("æ·±åº¦è¿½è¸ªï¼šinvoke æ–¹æ³•çš„å…·ä½“æ‰§è¡Œä»£ç ")
print("=" * 100)

# 3. æ‹¦æˆªå’Œå±•ç¤ºå…³é”®æ–¹æ³•çš„æ‰§è¡Œ
original_invoke = llm_with_tools.__class__.invoke
original_bound_invoke = llm_with_tools.bound.__class__.invoke
original_generate_prompt = llm_with_tools.bound.__class__.generate_prompt
original_generate = llm_with_tools.bound.__class__.generate
original_get_request_payload = llm_with_tools.bound._get_request_payload

step_counter = 1

def traced_runnable_binding_invoke(self, input, config=None, **kwargs):
    global step_counter
    print(f"\n{'='*60}")
    print(f"æ­¥éª¤ {step_counter}: RunnableBinding.invoke() æ‰§è¡Œ")
    print(f"{'='*60}")
    step_counter += 1
    
    print(f"è¾“å…¥å‚æ•°:")
    print(f"  - input: {input}")
    print(f"  - config: {config}")
    print(f"  - kwargs: {kwargs}")
    
    print(f"\nself.kwargs (ç»‘å®šçš„å·¥å…·ä¿¡æ¯):")
    for key, value in self.kwargs.items():
        if key == 'tools':
            print(f"  - {key}: {len(value)} ä¸ªå·¥å…·")
        else:
            print(f"  - {key}: {value}")
    
    # åˆå¹¶å‚æ•°çš„é€»è¾‘
    merged_kwargs = {**self.kwargs, **kwargs}
    print(f"\nåˆå¹¶åçš„ kwargs:")
    for key, value in merged_kwargs.items():
        if key == 'tools':
            print(f"  - {key}: {len(value)} ä¸ªå·¥å…·")
        else:
            print(f"  - {key}: {value}")
    
    print(f"\nå³å°†è°ƒç”¨: self.bound.invoke(input, merged_config, **merged_kwargs)")
    print(f"å…¶ä¸­ self.bound æ˜¯: {type(self.bound)}")
    
    # è°ƒç”¨åŸå§‹æ–¹æ³•
    return original_invoke(self, input, config, **kwargs)

def traced_chat_openai_invoke(self, input, config=None, **kwargs):
    global step_counter
    print(f"\n{'='*60}")
    print(f"æ­¥éª¤ {step_counter}: ChatOpenAI.invoke() æ‰§è¡Œ")
    print(f"{'='*60}")
    step_counter += 1
    
    print(f"æ¥æ”¶åˆ°çš„å‚æ•°:")
    print(f"  - input: {input}")
    print(f"  - config: {config}")
    print(f"  - kwargs keys: {list(kwargs.keys())}")
    
    if 'tools' in kwargs:
        print(f"  - tools: {len(kwargs['tools'])} ä¸ªå·¥å…·å·²ä¼ å…¥")
        print(f"    å·¥å…·å: {[tool['function']['name'] for tool in kwargs['tools']]}")
    
    print(f"\nå³å°†è°ƒç”¨: BaseChatModel.invoke() -> generate_prompt()")
    
    # è°ƒç”¨åŸå§‹æ–¹æ³•
    return original_bound_invoke(self, input, config, **kwargs)

def traced_generate_prompt(self, prompts, stop=None, callbacks=None, **kwargs):
    global step_counter
    print(f"\n{'='*60}")
    print(f"æ­¥éª¤ {step_counter}: generate_prompt() æ‰§è¡Œ")
    print(f"{'='*60}")
    step_counter += 1
    
    print(f"å‚æ•°:")
    print(f"  - prompts: {len(prompts)} ä¸ªæç¤º")
    print(f"  - kwargs keys: {list(kwargs.keys())}")
    
    if 'tools' in kwargs:
        print(f"  - tools: å·¥å…·ä¿¡æ¯ç»§ç»­ä¼ é€’")
    
    print(f"\nå³å°†è°ƒç”¨: self.generate() -> _generate()")
    
    # è°ƒç”¨åŸå§‹æ–¹æ³•
    return original_generate_prompt(self, prompts, stop, callbacks, **kwargs)

def traced_generate(self, messages, stop=None, callbacks=None, **kwargs):
    global step_counter
    print(f"\n{'='*60}")
    print(f"æ­¥éª¤ {step_counter}: generate() æ‰§è¡Œ")
    print(f"{'='*60}")
    step_counter += 1
    
    print(f"å‚æ•°:")
    print(f"  - messages: {len(messages)} æ‰¹æ¶ˆæ¯")
    print(f"  - kwargs keys: {list(kwargs.keys())}")
    
    if 'tools' in kwargs:
        print(f"  - tools: å·¥å…·ä¿¡æ¯ç»§ç»­ä¼ é€’åˆ° _generate()")
    
    print(f"\nå³å°†è°ƒç”¨: self._generate()")
    
    # è°ƒç”¨åŸå§‹æ–¹æ³•
    return original_generate(self, messages, stop, callbacks, **kwargs)

def traced_get_request_payload(self, input_, *, stop=None, **kwargs):
    global step_counter
    print(f"\n{'='*60}")
    print(f"æ­¥éª¤ {step_counter}: _get_request_payload() æ‰§è¡Œ")
    print(f"{'='*60}")
    step_counter += 1
    
    print(f"å‚æ•°:")
    print(f"  - input_: {input_}")
    print(f"  - stop: {stop}")
    print(f"  - kwargs keys: {list(kwargs.keys())}")
    
    if 'tools' in kwargs:
        print(f"  - tools: {len(kwargs['tools'])} ä¸ªå·¥å…·")
        print(f"    å³å°†æ·»åŠ åˆ° API payload ä¸­")
    
    # è°ƒç”¨åŸå§‹æ–¹æ³•è·å– payload
    payload = original_get_request_payload(self, input_, stop=stop, **kwargs)
    
    print(f"\næ„å»ºçš„ payload:")
    print(f"  - model: {payload.get('model')}")
    print(f"  - messages: {len(payload.get('messages', []))} æ¡æ¶ˆæ¯")
    
    if 'tools' in payload:
        print(f"  - tools: {len(payload['tools'])} ä¸ªå·¥å…· âœ“")
        print(f"    å·¥å…·å: {[tool['function']['name'] for tool in payload['tools']]}")
    else:
        print(f"  - tools: æœªæ‰¾åˆ°å·¥å…·ä¿¡æ¯ âœ—")
    
    print(f"\nè¿™ä¸ª payload å°†å‘é€ç»™ OpenAI API")
    
    return payload

# 4. åº”ç”¨æ‹¦æˆªå™¨
llm_with_tools.__class__.invoke = traced_runnable_binding_invoke
llm_with_tools.bound.__class__.invoke = traced_chat_openai_invoke
llm_with_tools.bound.__class__.generate_prompt = traced_generate_prompt
llm_with_tools.bound.__class__.generate = traced_generate
llm_with_tools.bound._get_request_payload = traced_get_request_payload

print("å¼€å§‹è¿½è¸ª invoke è°ƒç”¨...")

# 5. æ‰§è¡Œè°ƒç”¨
try:
    response = llm_with_tools.invoke("åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
    
    print(f"\n{'='*60}")
    print(f"æœ€ç»ˆç»“æœ")
    print(f"{'='*60}")
    
    print(f"å“åº”ç±»å‹: {type(response)}")
    print(f"å†…å®¹: {response.content}")
    
    if hasattr(response, 'tool_calls') and response.tool_calls:
        print(f"å·¥å…·è°ƒç”¨: {len(response.tool_calls)} ä¸ª")
        for tool_call in response.tool_calls:
            print(f"  - {tool_call['name']}: {tool_call['args']}")
    
    print(f"\nğŸ‰ å·¥å…·ä¿¡æ¯æˆåŠŸä» bind_tools() ä¼ é€’åˆ° OpenAI APIï¼")
    
except Exception as e:
    print(f"æ‰§è¡Œå‡ºé”™: {e}")

print(f"\n{'='*100}")
print("è¿½è¸ªå®Œæˆï¼šå·¥å…·ä¿¡æ¯ä¼ é€’çš„å®Œæ•´è·¯å¾„")
print(f"{'='*100}")

print("""
å·¥å…·ä¿¡æ¯ä¼ é€’è·¯å¾„æ€»ç»“:

1. bind_tools() é˜¶æ®µ:
   å·¥å…·å®šä¹‰ â†’ convert_to_openai_tool() â†’ RunnableBinding.kwargs['tools']

2. invoke() è°ƒç”¨é“¾:
   RunnableBinding.invoke()
   â†“ åˆå¹¶ self.kwargs (åŒ…å« tools) å’Œä¼ å…¥çš„ kwargs
   â†“ 
   ChatOpenAI.invoke(**merged_kwargs)  # tools åœ¨è¿™é‡Œ
   â†“
   BaseChatModel.invoke() â†’ generate_prompt() â†’ generate() â†’ _generate()
   â†“ kwargs ä¸­çš„ tools ä¸€è·¯ä¼ é€’
   â†“
   _get_request_payload(**kwargs)  # tools æœ€ç»ˆåˆ°è¾¾è¿™é‡Œ
   â†“
   æ„å»º API payloadï¼Œå°† tools æ·»åŠ åˆ°è¯·æ±‚ä¸­
   â†“
   å‘é€ç»™ OpenAI API

å…³é”®è§‚å¯Ÿï¼š
âœ“ å·¥å…·ä¿¡æ¯åœ¨æ¯ä¸ªæ­¥éª¤éƒ½æˆåŠŸä¼ é€’
âœ“ RunnableBinding èµ·åˆ°äº†å‚æ•°åˆå¹¶å’Œé€ä¼ çš„å…³é”®ä½œç”¨  
âœ“ æœ€ç»ˆåœ¨ _get_request_payload() ä¸­æ·»åŠ åˆ° API è¯·æ±‚
""")
